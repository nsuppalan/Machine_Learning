{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Visual Question Answering using Pythia\n",
    "[Pythia](https://github.com/facebookresearch/pythia) is a modular framework for vision and language multimodal research. Lets use it to have some fun solving [TextVQA](https://textvqa.org/). TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first some imports\n",
    "import yaml\n",
    "import cv2\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "import gc\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from pythia.utils.configuration import ConfigNode\n",
    "from pythia.tasks.processors import VocabProcessor, VQAAnswerProcessor\n",
    "from pythia.models.pythia import Pythia\n",
    "from pythia.common.registry import registry\n",
    "from pythia.common.sample import Sample, SampleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVQA:\n",
    "    TARGET_IMAGE_SIZE = [448, 448]\n",
    "    CHANNEL_MEAN = [0.485, 0.456, 0.406]\n",
    "    CHANNEL_STD = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._init_processors()\n",
    "        self.pythia_model = self._build_pythia_model()\n",
    "        #self.detection_model = self._build_detection_model()\n",
    "        #self.resnet_model = self._build_resnet_model()\n",
    "        \n",
    "    def _init_processors(self):\n",
    "        \"\"\"\n",
    "        Pythia uses processors is to keep data processing pipelines as similar as\n",
    "        possible for different datasets and allow code reusability.\n",
    "        \"\"\"\n",
    "        with open(\"configs/vqa/textvqa/lorra.yml\") as f:\n",
    "            config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        \n",
    "        #update config with includes for model specific config\n",
    "#         for inc in config.get(\"includes\", []):\n",
    "#             config.update(yaml.load(open(\"pythia/\"+ inc), Loader=yaml.FullLoader))\n",
    "\n",
    "        config = ConfigNode(config)\n",
    "    \n",
    "        registry.register(\"config\", config)\n",
    "        self.config = config\n",
    "        pprint.pprint(config.keys())\n",
    "        textvqa_config = config.task_attributes.vqa.dataset_attributes.textvqa\n",
    "        text_processor_config = textvqa_config.processors.text_processor\n",
    "        text_processor_config.params.vocab.vocab_file = \\\n",
    "            \"data/vocabs/vocabulary_100k.txt\"\n",
    "        answer_processor_config = textvqa_config.processors.answer_processor\n",
    "        answer_processor_config.params.vocab_file = \\\n",
    "            \"data/vocabs/answers_textvqa_8k.txt\"\n",
    "        self.text_processor = VocabProcessor(text_processor_config.params)\n",
    "        self.answer_processor = VQAAnswerProcessor(answer_processor_config.params)\n",
    "        registry.register(\"vqa2_text_processor\", self.text_processor)\n",
    "        registry.register(\"vqa2_answer_processor\", self.answer_processor)\n",
    "        registry.register(\"vqa2_num_final_outputs\", \n",
    "                      self.answer_processor.get_vocab_size())\n",
    "    \n",
    "    def _build_pythia_model(self):\n",
    "        state_dict = torch.load(\"data/models/pythia_train_val.pth\")\n",
    "        model_config = self.config.model_attributes.pythia\n",
    "        model_config.model_data_dir = \"./data\"\n",
    "        model = Pythia(mode_config)\n",
    "        model.build()\n",
    "        model.init_losses_and_metrics()\n",
    "        if list(state_dict.keys())[0].startswith('module') and \\\n",
    "        not hasattr(model, 'module'):\n",
    "            state_dict = self._multi_gpu_state_to_single(state_dict)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(\"cuda\")\n",
    "        model.eval()\n",
    "        return model\n",
    "    \n",
    "    def _build_resnet_model(self):\n",
    "        self.data_transforms = transforms.Compose([\n",
    "            transforms.Resize(self.TARGET_IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(self.CHANNEL_MEAN, self.CHANNEL_STD),\n",
    "        ])\n",
    "        resnet152 = models.resnet152(pretrained=True)\n",
    "        resnet152.eval()\n",
    "        modules = list(resnet152.children())[:2]\n",
    "        self.resnet152_model = torch.nn.Sequential(*modules)\n",
    "        self.resnet152_model.to(\"cuda\")\n",
    "        \n",
    "    def _multi_gpu_state_to_single(self, state_dict):\n",
    "        new_sd = {}\n",
    "        for k, v in state_dict.items():\n",
    "            if not k.startswith('module.'):\n",
    "                raise TypeError(\"Not a multiple GPU state of dict\")\n",
    "            k1 = k[7:]\n",
    "            new_sd[k1] = v\n",
    "        return new_sd\n",
    "\n",
    "    def predict(self, url, question):\n",
    "        return\n",
    "#         with torch.no_grad():\n",
    "#             detectron_features = \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['includes', 'task_attributes', 'model_attributes', 'optimizer_attributes', 'training_parameters'])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "text_processor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-7013dbdac94e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_vqa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextVQA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-124-3b82207e5be8>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_processors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpythia_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_pythia_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#self.detection_model = self._build_detection_model()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-124-3b82207e5be8>\u001b[0m in \u001b[0;36m_init_processors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mpprint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mtextvqa_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_attributes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvqa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_attributes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextvqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mtext_processor_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtextvqa_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_processor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mtext_processor_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_file\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;34m\"data/vocabs/vocabulary_100k.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\github\\pull\\pythia\\pythia\\utils\\configuration.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: text_processor"
     ]
    }
   ],
   "source": [
    "text_vqa = TextVQA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13.1",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
