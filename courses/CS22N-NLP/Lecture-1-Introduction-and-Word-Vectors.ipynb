{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Word Vectors\n",
    "\n",
    "- Two thoughts of Prof Christopher Manning  about language:\n",
    "    - Language is such an evolved system of communication, but very **uncertain**.\n",
    "        - However humans have some agreed meaning which helps us communicate so well.\n",
    "        - We are internally and subconsciously doing some kind of probabilistic inference to determine meaning not just for information but also for social functions etc.\n",
    "    - For artificial intelligence to reach at a very sophisticated level, it needs to be able to capture all of the human knowledge, which is predominantly conveyed through human language. \n",
    "        - Human language is our networking language through which we collectively form a huge network of individuals.\n",
    "        - Human language made human being invincible. Language made humans to be able to work collectively as a group or team.  That is how they evolved not to just survive in a world of more powerful animals but they thrived.\n",
    "        - Invention of writing made this knowledge to shared spatially (i.e. through space) or temporally (i.e. through time) not just verbally. \n",
    "        - Writing is very recent (~5000 years) phenomenon in scale of evolution, but made humans super powerful.\n",
    "        - We compress knowledge efficiently and provide a view of the world in very few bits of information (e.g. I went to Zoo and saw an elephant. When you read this it constructs the whole visual scenery in your mind with images which can take few megabytes to store in a computer, but was communicated in very little words).\n",
    "\n",
    "### How do we represent the meaning of the words?\n",
    "- Linguists use something called Denotational Semantics to think about meaning.\n",
    "    - Linguists think of meaning as what things represent.\n",
    "    - $$\\text{signifier(symbol)}  \\leftrightarrow \\text{signified{(idea or thing)}}$$\n",
    "    - Word \"chair\" representing all the thing that are chair.\n",
    "    - Word \"running\" represents a set of actions people do, which represents the activity those actions perform i.e. 🏃‍♂️ \"\n",
    "\n",
    "### How do we have reasonable meaning in a computer?\n",
    "- Common Solution: Something like `WordNet` which is a thesaurus containing words and their relationships using  synonym set and hypernyms (\"is a\" relationship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ramand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Synonyms of \"good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun : good\n",
      "noun : good, goodness\n",
      "noun : good, goodness\n",
      "noun : commodity, trade_good, good\n",
      "adjective : good\n",
      "adjective(s) : full, good\n",
      "adjective : good\n",
      "adjective(s) : estimable, good, honorable, respectable\n",
      "adjective(s) : beneficial, good\n",
      "adjective(s) : good\n",
      "adjective(s) : good, just, upright\n",
      "adjective(s) : adept, expert, good, practiced, proficient, skillful, skilful\n",
      "adjective(s) : good\n",
      "adjective(s) : dear, good, near\n",
      "adjective(s) : dependable, good, safe, secure\n",
      "adjective(s) : good, right, ripe\n",
      "adjective(s) : good, well\n",
      "adjective(s) : effective, good, in_effect, in_force\n",
      "adjective(s) : good\n",
      "adjective(s) : good, serious\n",
      "adjective(s) : good, sound\n",
      "adjective(s) : good, salutary\n",
      "adjective(s) : good, honest\n",
      "adjective(s) : good, undecomposed, unspoiled, unspoilt\n",
      "adjective(s) : good\n",
      "adverb : well, good\n",
      "adverb : thoroughly, soundly, good\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "poses = {'n' : 'noun', 'v' : 'verb', 'a' : 'adjective', 's' : 'adjective(s)', 'r' : 'adverb' }\n",
    "for synset in wn.synsets(\"good\"):\n",
    "    print(f'{poses[synset.pos()]} : {\", \".join([l.name() for l in synset.lemmas()])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hypernyms of \"Tiger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('person.n.01'),\n",
       " Synset('causal_agent.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('entity.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiger = wn.synset(\"Tiger.n.01\")\n",
    "hyper = lambda t: t.hypernyms()\n",
    "list(tiger.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- WordNet is outlining various use of \"good\" in English. They are very fine grained difference which humans can barely understand.\n",
    "- It clearly misses nuance. e.g. expert is not really \"good\"\n",
    "- It also misses new meanings of word e.g. 'wicked good'. It is based on human labor and impossible to keep up-to-date.\n",
    "- It also can't give us accurate word similarity and a score of how similar a pair of words are?\n",
    "- It is very subjective.\n",
    "\n",
    "\n",
    "### Localist Distribution\n",
    "This is tangential to current discussion.\n",
    "- A representation of space or collection where each entity is represented independently in a space.\n",
    "- It can only describe a number of distinct object that are linear in number of dimension.\n",
    "- This representation do not represent any relationship between the entities.\n",
    "- One Hot Encoding is a localist representaiton.\n",
    "- If we represent each word as a symbol, English language is estimated to have 13 million words. If we represent each word as a vector of 13 million dimension with one 1 and rest 0.\n",
    "- In Neurology, localist representation theorizes that each neuron is a single concept on a stand alone basis. Each neuron or localist unit which has \"meaning and representation\"\n",
    "- This is inverse of Distributed Representation.\n",
    "\n",
    "\n",
    "### Representing words as discrete symbols.\n",
    "- **Pre-2012**\n",
    "    - Words as discrete symbols in lexicon (vocabulary)\n",
    "    - \"hotel, conference, motel\" a localist representation\n",
    "    - One Hot Encoding is used to represent the words as vector\n",
    "        - motel = [00000100]\n",
    "        - hotel =  [00000010]\n",
    "    - These vectors become huge because languages have lot of words.\n",
    "    - In language like English, we can have almost infinite words by using Derivation Morphology \n",
    "        - \"New words are created in language by adding more words to the ending of existing words.\"\n",
    "        - \"e.g. Paternal --> Paternalistic, Paternalistically \"\n",
    "        - \"This can explode the vocabulary of a language by many folds.\"\n",
    "    - This takes huge computational power as word vector can be of dimension 500,000 or more.\n",
    "    - Another bigger problem is often times we are interested in relationship and meaning of words.\n",
    "        - If I search for \"Seattle Motels\", I might also actually like \"Seattle Hotels\" in the search too.\n",
    "        - However,  words as a symbol representation keeps these words orthogonal in the space. See One Hot Encoding above. \n",
    "        - There is no notion of similarity between one-hot encoded vectors.\n",
    "        - Word Similarity tables can solve this problem but that means but that explodes the computational problem. For each pair of words you keep a score of how similar they are but leads to really large table e.g.  using 500,000 words in vocabulary we might end up with a table of 2.5 trillion cells.\n",
    "- Instead of that how about **we encode \"similarity\" in vector themselves?**\n",
    "\n",
    "### Distributional Semantics\n",
    "- Linguistic meaning of the word: A word's meaning is given by the words which appear close to this word frequently. \n",
    "- When a word *w* appears in the text, it's **context** is the set of words that appear near *w*  at a fixed length window.\n",
    "- \"You shall know a word by the company it keeps\" - J. R. Firth 1957\n",
    "- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fdailygrind%2FlUwgP_m7kQ.png?alt=media&token=933154b6-2990-4d0a-aa17-323b7ddabf13)\n",
    "- The meaning of \"banking\" is the collections of all the words around it.\n",
    "\n",
    "\n",
    "### Distributed Representation\n",
    "\n",
    "- This idea is inverse of localist representation e.g. One Hot Encoding.\n",
    "- In One Hot Encoding each vector is independent of the other words and represented as really large vectors (English has 13 million different words) where each vector has all 0s but one 1 which represent that word.\n",
    "    - motel = \\[00000100...00\\]\n",
    "    - hotel  = \\[00000010...00\\]\n",
    "- In distributed representation, each word is represented as a dense vector which is similar to vector of words that appear in similar contexts.\n",
    "- In other words, words which appear together live closer in the vector space representing all the words. e.g. motel and hotel will be related words and will live close in the vector space.\n",
    "- The dimensions of this vector is very small compared to the vocabulary e.g. 50, 100, 200.... 4000 as compared to 13 million English words. \n",
    "- We use this smaller vector space to encode the relationship between words.\n",
    "$$ \\text{banking} = \\begin{pmatrix} 0.102 \\\\ 0.432 \\\\0.445\\\\ 0.001\\\\0.034 \\end{pmatrix}$$\n",
    "- These word vectors are sometimes called Word Embeddings or word representations.\n",
    "\n",
    "- When this vector space of some large dimension (100) is squished or projected to a 2-D plane, although you lose lot of information but still the related words seemingly appear together. \n",
    "    - A cluster of countries in one such projected plane\n",
    "    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fdailygrind%2F6feMskbybU.png?alt=media&token=7147604c-ac91-4612-bdc3-c1917e6c9db1)\n",
    "- ### Word2Vec \n",
    "    - By Tomas Mikolov\n",
    "    - **Idea:**\n",
    "        - We have a sufficiently large corpus of text.\n",
    "        - Initially each word in this fixed set of vocabulary is represented by a vector with arbitrary values.\n",
    "        - We then iterate through corpus, for each position *t* in text\n",
    "            - We have the center word *c*\n",
    "            - We also have context words *o* (\"outside\" words surrounding *c*)\n",
    "            - Use the similarity of the word vectors for *c * and *o* to calculate the probability of *o* given *c* (or vice versa)\n",
    "                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fdailygrind%2FFvXzkI7qYs.png?alt=media&token=40ab1db5-7de2-4de2-add1-88d8c9e8fe71)\n",
    "            - Keep adjusting the word vectors to **maximize this probability.** (covered below)\n",
    "            - This is repeated for each position.\n",
    "                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fdailygrind%2FoQ83JB0O6A.png?alt=media&token=aa676c96-1731-4295-bd5c-034b8816a42b)\n",
    "- **Types of word2vec algorithms**\n",
    "    - Skip-Grams (SG)\n",
    "        - Predict context words given target irrespective of position of context words.\n",
    "    - Continuous Bag of words (CBOW)\n",
    "        - Predict target word from bag-of-words context\n",
    "- **Skip-gram Prediction**\n",
    "    - For every word t = 1...*T*, predict surrounding context words in a window of radius \"m\" of every words.\n",
    "    - **Objective function**: Maximize the probability of any context word given the current center word. \n",
    "    - It is also called loss or cost function.\n",
    "    - $\\text{Likelihood} = L(\\theta) = \\displaystyle\\prod_{t=1}^{T}\\prod_{-m \\leq j \\leq m \\atop j \\neq 0} P(w_{t+j} | w_t; \\theta)$\n",
    "    - What $L(\\theta)$ is saying is that we have a large corpus of text, we are going to go through each position in this text, and for each such position, we are going to a have window of *2m* around it (*m *  words before, *m * words after), and then we are going to have a probability distribution that will give us probability to a word appearing in context of the center word.\n",
    "    - Context words are represented as \"O\" (outside words?) and center word is represented as \"C\".\n",
    "    - We would like to set the parameters of the model such that, these probabilities of the words that do appear in context of the center word is as high as possible.  $\\theta$ is the parameter of our model, which is the vector representation of the word.\n",
    "    - Likelihood is then representing how good a job our model will do in predicting words around every word!\n",
    "    - Representing the objective function in more math friendly way (negative log likelihood)\n",
    "    - $ J(\\theta) = -\\frac{1}{T} log L(\\theta) = -\\frac{1}{T} \\displaystyle\\sum_{t=1}^{T}\\sum_{-m \\leq j \\leq m \\atop j \\neq 0} logP(w_{t+j} | w_t; \\theta)$\n",
    "    - Minimizing the objective function $\\leftrightarrow$ Maximizing predictive accuracy\n",
    "\n",
    "- **How do we calculate the probability?**\n",
    "    - Each word will be represented by two vectors. $V$ and $U$\n",
    "    - $V$ represents the word as center word and $U$ represents the word as a context word.\n",
    "    - $U^{T}V$ represents how similar the center word is to another context word. It is simple dot product. It will be bigger if U and V are similar.\n",
    "    - So if we iterate over all words $\\displaystyle\\sum_1^W U_w^TV$\n",
    "        We are basically working out how similar each word is to V.\n",
    "    - **Softmax form**\n",
    "        - The dot product of vectors we obtained can be any number say -304.\n",
    "        - Softmax is the standard way to convert these numbers to a probability distribution.\n",
    "        - First we expontentiate them. That makes the values positive. It amplifies the max, but still assign values to lower values.\n",
    "        - Then we normalize to convert them to probability by summing all the numbers together and divide it by the value.\n",
    "    - Finally, the ratio is representing what is the probability of word at index o in vocabulary to appear in context of the current center word (word at index c in vocabulary)\n",
    "        - $P(o|c) = \\displaystyle\\frac{exp(U_{o}^T V_{c})}{\\sum_1^W exp(U_w^TV_{c})}$\n",
    "            - Where *o* is the outside (or output) word index, and *c* is the center word index, $V_{c}$ and $U_{o}$ are \"center\" and \"outside\" vectors of indices *c* and *o*\n",
    "            - Understand the difference between $P(w_{t+j} | w_t; \\theta)$ and $P(o|c)$\n",
    "                *c* and *o* are indices in the space of vocabulary (e.g. word 218 and word 3023 in the vocabulary), while *t* and *t+j* are positions in the text (e.g. word 342 and 346 in the text corpus)\n",
    "- **How it all fits together?**\n",
    "    - As discussed above, we will have two vectors $V_c$ and $U_o$ representing the word. $V_c$ represents the word as center word, while $U_o$ represents the word as \"outside\" or context word.\n",
    "    - Initially these vectors are assigned random values. Our goal is to find the right values of these vectors i.e. to train the model and compute the best possible values for these vectors. These are the parameters of the model.\n",
    "    - We often define the set of all parameters in a model in a one long vector $\\theta$\n",
    "        - $\\theta = \\begin{bmatrix} V_{\\text{aardvark}} \\\\ V_{\\text{a}} \\\\ . \\\\ . \\\\ . \\\\ V_{\\text{Zyzzyva}} \\\\ U_{\\text{aardvark}} \\\\ U_{\\text{a}} \\\\ . \\\\ . \\\\ . \n",
    "\\\\ U_{\\text{Zyzzyva}} \\end{bmatrix} \\in {\\rm I\\!R}^{2dV}$\n",
    "        - Each vector has dimension of $d$ and there are $2V$ vectors with $V$ is the vocabulary size.\n",
    "    - Essentially, we want to minimize the overall loss i.e. the negative log likelihood, we saw above. At minimum loss we will have the best possible value of $\\theta$.\n",
    "\n",
    "- **How do we minimize the negative log likelihood and how does it give us the best** $\\theta$ **values?**\n",
    "    - The objective function can be rewritten as:\n",
    "        - $ J(\\theta) = \\displaystyle-\\frac{1}{T} log L(\\theta) = -\\frac{1}{T} \\sum_{t=1}^{T}\\sum_{-m \\leq j \\leq m \\atop j \\neq 0} log\\frac{exp(U_o^TV_c)}{\\sum_{w=1}^vexp(U_w^TV_c)}$\n",
    "    - We will minimize it by calculating it's gradient value $\\frac{dJ(\\theta)}{d\\theta}$ \n",
    "    - Since $\\theta$ is built of $V_c$ and $U_o$, we will essentially calculate $\\frac{\\partial J(\\theta)}{\\partial V_c}$ and $\\frac{\\partial J(\\theta)}{\\partial U_o}$\n",
    "    - Let's first focus on $\\frac{\\partial J(\\theta)}{\\partial V_c}$ \n",
    "    - Ignoring the summation and constant terms we basically need to calculate\n",
    "        -  $\\frac{\\partial J(\\theta)}{\\partial V_c } = \\frac{\\partial}{\\partial V_c} log\\frac{exp(U_o^TV_c)}{\\sum_{w=1}^vexp(U_w^TV_c)} $\n",
    "        - $= \\frac{\\partial}{\\partial V_c} log (exp(U_o^TV_c)) - \\frac{\\partial}{\\partial V_c} log \\sum_{w=1}^v exp(U_w^TV_c)$\n",
    "        - $ = t_1 - t_2$\n",
    "    - Let's tackle the first term first. \n",
    "        - $t_1 = \\frac{\\partial}{\\partial V_c} log(exp(U_o^TV_c)) = \\frac{\\partial U_o^TV_c}{\\partial V_c}  = U_o$\n",
    "    - Let's tackle the second term.\n",
    "        - $t_2 = \\frac{\\partial}{\\partial V_c} log \\sum_{w=1}^v exp(U_w^TV_c)$\n",
    "        - $t_2 = \\frac{1}{\\sum_{w=1}^v exp(U_w^TV_c)} \\frac{\\partial}{\\partial V_c}\\sum_{x=1}^v exp(U_x^TV_c)$\n",
    "        - Notice the change from $w$ to $x$ to avoid confusing this summation with denominator\n",
    "        - Move the derivative inside the summation.\n",
    "        - $t_2 = \\frac{1}{\\sum_{w=1}^v exp(U_w^TV_c)} \\sum_{x=1}^v \\frac{\\partial}{\\partial V_c}exp(U_x^TV_c)$\n",
    "        - $t_2 = \\frac{1}{\\sum_{w=1}^v exp(U_w^TV_c)} \\sum_{x=1}^v exp(U_x^TV_c)U_x$\n",
    "        - Moving the denominator inside the summation too.\n",
    "        - $t_2 = \\sum_{x=1}^v \\frac{exp(U_x^TV_c)U_x}{\\sum_{w=1}^v exp(U_w^TV_c)}$\n",
    "        - The partial inner term has pattern $P(x|c) = \\displaystyle\\frac{exp(U_{x}^T V_{c})}{\\sum_1^W exp(U_w^TV_{c})}$\n",
    "        - Therefore $t_2 = \\sum_{x=1}^v P(x|c)U_x$\n",
    "    - Going back to original equation\n",
    "        - $\\frac{\\partial J(\\theta)}{\\partial V_c }= t_1 - t_2$\n",
    "        - $\\frac{\\partial J(\\theta)}{\\partial V_c }= U_o - \\sum_{x=1}^v P(x|c)U_x$\n",
    "    - Notice how amazing this result is. $t_1$ essentially represents the observed value and second term $t_2$ has form of an expectation value, we are calculating probability of each word in vocabulary appearing in context of the center word, and then taking that probability and multiplying with it's current context vector. In other words, it is average over all possible context vectors weighted by likelihood of their occurrence.\n",
    "    - We are essentially substracting expected value from observed value and trying to bring them close.\n",
    "    - Similarly by symmetry:\n",
    "        - $\\frac{\\partial J(\\theta)}{\\partial U_o }= t_1 - t_2$\n",
    "        - $\\frac{\\partial J(\\theta)}{\\partial V_c }= V_c - \\sum_{x=1}^v P(x|c)V_x$\n",
    "    - This way we have calculated the gradient. Substracting a fraction of gradient moves the value towards minimum.\n",
    "    - Update of the parameters happen in the following manner.\n",
    "        - $\\theta_{j}^{new} = \\theta_{j}^{old} - \\alpha\\frac{\\partial}{\\partial\\theta_{j}^{old}} J(\\theta)$\n",
    "        - In matrix form: $\\theta^{new} = \\theta^{old} - \\alpha\\frac{\\partial}{\\partial\\theta^{old}} J(\\theta)$\n",
    "        - or $\\theta^{new} = \\theta^{old} - \\alpha\\triangledown_\\theta J(\\theta)$\n",
    "        - $\\alpha$ is our step size which is used to decide how much we move in each step.\n",
    "    - This is essentially [[Gradient Descent]] algorithm, which we will come back to separately.\n",
    "    - We actually use [[Stochastic Gradient Descent]] to avoid exceedingly big computation and in fact a better result.\n",
    "    - Instead of updating $\\theta$ using entire vocabulary, we update parameter after each window $t$ (basically update the parameter for each individual center word.)\n",
    "        - $\\theta^{new} = \\theta^{old} - \\alpha\\triangledown_\\theta J_t(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Word2Vec from scratch using numpy\n",
    "\n",
    "Let's implement word2Vec from scratch. I have implemented it in another notebook [here](Word2Vec_from_scratch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
